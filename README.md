# InputGazer
This library aims to use Eye Tracking to input Num keys on a webpage, demonstrating the feasability of eye-controlled web browsing

Demo

![Video Demo](https://github.com/KP021/InputGazer/blob/main/images/gif_video_demo.gif)



Files:

To correctly initialize and run the tools, the following files and folders are needed
- keypad.html
- keypad.css
- js folder 

To run the tool, it will be necessary to initialize a local http server

![Console server](https://github.com/KP021/InputGazer/blob/main/images/console.png)

Camera permissions will be needed to ensure tool function.
The user will have to calibrate on the points provided before using the numpad. A calibration of 27-45 clicks, 3-5 per point, is suggested.

# Write-Up



## Data collection
All data was collected on an iPhone X using SensorLog, an iOS app on the App Store. Data was recorded with the participation of 12+ individuals over the course of one month using each platform on iOS who have experience using each platform to account for variability. All participants are Carnegie Mellon University undergraduate students. However, most data was generated by two individuals due to time constraints and recruitment over holidays. We tested on five different social media platforms:

Instagram
Tinder
Facebook
LinkedIn
Notetaking/texting
Samples for activity on each social media platform are approximately between 30-60 seconds long. Subjects were told to use the platforms as they normally would, and were instructed when to start and stop recordings. It is worth noting no data was collected while traveling.


![Numpad](https://github.com/KP021/InputGazer/blob/main/images/numpad.png)
![Calibration](https://github.com/KP021/InputGazer/blob/main/images/calibration.png)


## Model Structure
Add Shivin & Irene slides

## Pre-processing
Recordings were sampled at 30Hz, with sensor data including location, acceleromter, gyroscope, magnetometer, motion and rotation, and altimeter amongst other data. I removed unnecessary metadata from the dataset. Generally speaking, there was a brief switchover time of 1-2 seconds when users flipped between the sensor app and social media platform. We removed two seconds from the beginning and end of each recording to ensure all data collected was exclusively the activity taking place.

We tried multiple methods to pre-process the data, including a low-pass filter, median filter, and combination of both to reduce the noise. There was no significant difference in accuracy with the combination of both, and the low-pass filter performed better. Windowing was similar. Activity recognition was siginficantly better when windowing in general, but window sizes of 1s, 1.5s, 2s, 2.5s, 3s, 3.5s, and 4s with 50% overlap showed no significant differences. Because user actions on each platform likely don't change much, this was a somewhat effective way to increase the amount of data available. Windowing is also much better for implementing our program in real-time.

## Feature extraction
We started by extracting typical features after exploratory data anlysis and plotting of the different social media platforms. The common features include extremes, averages, and variance such as standard deviation. Calculated features include the norm of 3-axis sensors and rootmean-square.

Domain-specific features for accelerometer and gyroscope data:

Extremes (max, min)
Averages (mean, median)
Variance (st. deviation, rootmean-square)
Feature selection
We selected features by comparing the different classes to each other using summary statistics and graphing data such as the accelerometer and gyroscope. The accelerometer and gyroscope data were the most consistent features. It was hard to empirically select at times because the data varies per participant and trial. Consequently, we also used a feature ranking algorithm to determine which features were more useful.

## Classification
We tried two approaches to classification: Decision trees (+ others) and Concurrent neural network.

Decision Trees & others
Because of finding success in activity recognition of A4 through decision trees, we started with a Random Forest classifier and managed to achieve about 80% with basic features. We did however, try a variety of other classifiers.

Decision Tree (78%)
KNN (60% accuracy)
SVM (62% accuracy)
MLP (64% accuracy, failed to converge)
Logistic (20% accuracy)
Bayesian (~20% accuracy)
As it turns out, logistic and bayesian classifiers were no better than guessing, and trees performed the best for our activities. We then looked to another method to see if we could attain better results.

Neural Networks
We also tried a Convolutional Neural Network (CNN) to see if an algorithm could generate better features than ourselves and achieve over 80% accuracy. CNNs essentially have three parts: convolution layers, pooling layers, and fully-connected layers.

## Testing
In response to your question in class about testing on different users, we individually collected 10 thirty-second samples for every class and trained two separate models and cross-tested. When testing on themselves, the same model above yielded > 95% accuracy in both cases. However, when applied to an entirely different user was only about 10% on average--which is worse than chance.

Mean accuracy testing on self, subject 1: 0.9520730053906185
Mean accuracy testing on self, subject 2: 0.9793786240642613

Testing subject 2 on subject 1 model:
Mean accuracy: 0.13247746517344988

Testing subject 1 on subject 2 model:
Mean accuracy: 0.09002072845721054
This test demonstrates that people use social media in much different ways due to a variety of technological and physiological factors. This may include how users hold their phones, hand dominance, posture, attention-span, use-cases, etc. It is better to train a model like this on the user himself, or perhaps even attempt to generalize with an enormous dataset from many users.

## Conclusion
Detecting minute differences in sensor data to distinguish social media platforms has a lot of potential to add value to companies such as Microsoft or Apple, and an interesting task. Of course, all users are much different from one another, which was shown in our recent test. Recognizing different social media platform usage with an iPhoneâ€™s motion sensors (accelerometer, gyroscope, etc) can certainly be improved, or achieved successfully if trained on individual users. Because detecting usage of these platforms would be unnoticable to any human, we're fairly happy with the conclusion that it is conditionally possible to make accurate predictions, and would be interested to see if a more generalized model might work.
